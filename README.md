# Weibo Comment Analytics & TopicGPT Project

This project provides a complete toolkit for crawling, structuring, and analyzing comments from Weibo posts. It consists of a robust crawler that captures multi-level discussions and an interactive Streamlit dashboard for visualizing the results.

## Project Structure

```
2_topicGPT_with_seed/
├── weibo_comments_2nd.py        # The Crawler: Fetches comments from m.weibo.cn
└── topicgpt_results/
    ├── app.py                   # The Dashboard: Streamlit app for visualization
    └── mobile_comments_structured.csv  # Output: The data file generated by the crawler
```

---

## Part 1: Data Collection (`weibo_comments_2nd.py`)

This script visits the mobile version of Weibo (`m.weibo.cn`) to scrape comments from a specific post. It effectively handles nested conversations, preserving the context of who replied to whom.

### Key Features
- **Deep Crawling**: Fetches both "Root" comments (Level 1) and "Reply" comments (Level 2).
- **Structure Parsing**: Intelligently identifies "Reply-To" relationships (e.g., extracting ` @User` from text).
- **Data Cleaning**: Removes HTML tags and formats timestamps.
- **Anti-Bot Measures**: Uses random sleep intervals to mimic human behavior.

### Usage

1. **Get your Cookie**:
   - Log in to [m.weibo.cn](https://m.weibo.cn) in your browser.
   - Open Developer Tools (F12) -> Network.
   - Refresh the page and find a request to the domain.
   - Copy the value of the `Cookie` header.

2. **Find the Post ID**:
   - The ID is the numeric string at the end of the post URL (e.g., `https://m.weibo.cn/detail/123456789`).

3. **Run the script**:
   ```bash
   python weibo_comments_2nd.py
   ```
   
4. **Follow the prompt inputs**:
   - Paste your **Mobile Cookie**.
   - Input the **Target Post ID**.

> **Output**: The data will be saved to `topicgpt_results/mobile_comments_structured.csv`.

---

## Part 2: Visualization Dashboard (`app.py`)

The Streamlit application turns the raw CSV data into actionable insights using interactive charts and word clouds.

### Key Features
- **Key Metrics**: Total comments, active users, engagement rates (likes).
- **Activity Timeline**: View comment volume trends over time.
- **User Network**: Identify top active users and most liked content.
- **Word Cloud**: Visualizes high-frequency terms (using `jieba` Chinese segmentation) with built-in stopword filtering.
- **Flexible Loading**: Automatically loads the crawler's output or accepts a CSV upload via drag-and-drop.

### Usage

Run the following command from the project root or the `topicgpt_results` folder:

```bash
streamlit run topicgpt_results/app.py
```

The dashboard will open in your default web browser (usually at `http://localhost:8501`).

---

## Installation & Requirements

Ensure you have Python installed. Install the necessary libraries:

```bash
pip install streamlit pandas plotly wordcloud matplotlib jieba requests
```

### Dependencies
- **Data Handling**: `pandas`, `numpy`
- **Visualization**: `streamlit`, `plotly`, `matplotlib`, `wordcloud`
- **NLP**: `jieba` (for Chinese text segmentation)
- **Network**: `requests`

---

## Notes

- **Fonts**: The WordCloud generator attempts to use system fonts like `PingFang.ttc` (macOS) or falls back to `SimHei.ttf` if available in the directory. If you see squares in the word cloud, ensure a Chinese font is available.
- **Rate Limiting**: Weibo has strict rate limits. The crawler sleeps between requests to avoid being blocked. If scraping large posts, it may take some time.
